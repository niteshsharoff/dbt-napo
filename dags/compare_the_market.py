import pendulum
from airflow.datasets import Dataset
from airflow.decorators import task
from airflow.models import Variable

from airflow.models.dag import dag
from airflow.operators.empty import EmptyOperator

from dags.workflows.export_bq_result_to_gcs import export_table_to_gcs
from dags.workflows.upload_to_google_drive import upload_to_google_drive
from workflows.create_bq_view import create_ctm_sales_monthly_view
from workflows.create_bq_external_table import create_external_bq_table

OAUTH_TOKEN_FILE = Variable.get("OAUTH_CREDENTIALS")
GCP_PROJECT_ID = Variable.get("GCP_PROJECT_ID")
GCP_REGION = Variable.get("GCP_REGION")
GCS_BUCKET = Variable.get("GCS_BUCKET")

GOOGLE_DRIVE_DAILY_FOLDER_ID = "1JEtPgxP38MWYLaxgZRNwJLkziYTRtRHf"
GOOGLE_DRIVE_MONTHLY_FOLDER_ID = "1iK37ZMa_9dDxkdxgVD9KSNInEsmzRTaR"
BQ_DATASET = "reporting"
DAILY_TABLE = "ctm_sales_report_daily"
MONTHLY_TABLE = "ctm_sales_report_monthly"
TMP_TABLE = "tmp"


@task(
    task_id="create_daily_table",
    outlets=[Dataset("reporting.ctm_sales_report_daily")],
)
def update_daily_table():
    """
    This task creates / updates an external Big Query table on top of the generated
    daily ctm sales report partitioned by run date. The sales report is currently
    generated by policy service on a reporting instance.

    The Created Big Query table is:
        ae32-vpcservice-datawarehouse.reporting.ctm_sales_report_daily

    """
    create_external_bq_table(
        project_name=GCP_PROJECT_ID,
        region=GCP_REGION,
        dataset_name=BQ_DATASET,
        table_name=DAILY_TABLE,
        source_uri=f"gs://{GCS_BUCKET}/tmp/ctm/*",
        partition_uri=f"gs://{GCS_BUCKET}/tmp/ctm",
        source_format="CSV",
        schema_path=None,
        skip_leading_rows=0,
    )


@task(task_id="upload_daily_report")
def upload_daily_report(ds=None):
    """
    This task uploads a daily report to a shared Google Drive folder in csv format.
    The report day is the same as the ETL run_date but the data should be from the
    day prior.

    The Google shared drive folder is:
        https://drive.google.com/drive/folders/1JEtPgxP38MWYLaxgZRNwJLkziYTRtRHf

    """
    run_date = pendulum.parse(ds, tz="UTC")
    report_date = run_date.subtract(days=1)
    gcs_file_name = "100161_Pet_{0}_{0}_1_2.csv".format(report_date.format("DDMMYYYY"))
    gdrive_file_name = f"{DAILY_TABLE}_{run_date.format('YYYYMMDD')}.csv"
    upload_to_google_drive(
        project_name=GCP_PROJECT_ID,
        gcs_bucket=GCS_BUCKET,
        gcs_path=f"tmp/ctm/run_date={run_date.date()}/{gcs_file_name}",
        gdrive_folder_id=GOOGLE_DRIVE_DAILY_FOLDER_ID,
        gdrive_file_name=gdrive_file_name,
        token_file=OAUTH_TOKEN_FILE,
    )


@task.branch(task_id="monthly_branch")
def is_first_of_month(ds=None):
    """
    Branch and run monthly tasks only on the first day of each month.
    Daily runs not on the first of the month should branch to no op.
    """
    run_date = pendulum.parse(ds, tz="UTC")
    if run_date.day == 1:
        return "create_monthly_view"

    return "no_op"


@task(
    task_id="create_monthly_view",
    outlets=[Dataset("reporting.ctm_sales_report_monthly_*")],
)
def update_monthly_view(ds=None):
    """
    This task creates a Big Query monthly view on top of the ctm_sales_report_daily
    table.

    The monthly views are named in the following format:
        ae32-vpcservice-datawarehouse.reporting.ctm_sales_report_monthly_{YYYYMMDD}

    """
    run_date = pendulum.parse(ds, tz="UTC")
    start_date = pendulum.datetime(run_date.year, run_date.month - 1, 1, tz="UTC")
    end_date = pendulum.datetime(run_date.year, run_date.month, 1, tz="UTC")
    create_ctm_sales_monthly_view(
        project_name=GCP_PROJECT_ID,
        dataset_name=BQ_DATASET,
        src_table=DAILY_TABLE,
        view_name=f"{MONTHLY_TABLE}_{start_date.format('YYYYMMDD')}",
        start_date=start_date,
        end_date=end_date,
    )


@task(task_id="export_monthly_report")
def export_monthly_view(ds=None):
    """
    This task query's the monthly view table, stores the results in a temp table and
    exports the result to Cloud Storage.

    The weekly view table is:
        ae32-vpcservice-datawarehouse.reporting.ctm_sales_report_monthly_{YYYYmmdd}

    The results are stored in a tmp table which expires after 1 hour:
        ae32-vpcservice-datawarehouse.reporting.tmp

    """
    run_date = pendulum.parse(ds, tz="UTC")
    start_date = pendulum.datetime(run_date.year, run_date.month, 1, tz="UTC")
    table_name = f"{MONTHLY_TABLE}_{start_date.format('YYYYMMDD')}"
    export_table_to_gcs(
        project_name=GCP_PROJECT_ID,
        dataset_name=BQ_DATASET,
        src_table=table_name,
        tmp_table=TMP_TABLE,
        gcs_uri=f"gs://{GCS_BUCKET}/{BQ_DATASET}/{MONTHLY_TABLE}/{table_name}.csv",
    )


@task(task_id="upload_monthly_report")
def upload_monthly_report(ds=None):
    """
    This task uploads the monthly report to a shared Google Drive folder in csv format.
    The report month is the same as the ETL run_date but the data should be from the
    month prior.

    The Google shared drive folder is:
        https://drive.google.com/drive/folders/1iK37ZMa_9dDxkdxgVD9KSNInEsmzRTaR

    """
    run_date = pendulum.parse(ds, tz="UTC")
    start_date = pendulum.datetime(run_date.year, run_date.month - 1, 1, tz="UTC")
    file_name = f"{MONTHLY_TABLE}_{start_date.format('YYYYMMDD')}.csv"
    upload_to_google_drive(
        project_name=GCP_PROJECT_ID,
        gcs_bucket=GCS_BUCKET,
        gcs_path=f"{BQ_DATASET}/{MONTHLY_TABLE}/{file_name}",
        gdrive_folder_id=GOOGLE_DRIVE_MONTHLY_FOLDER_ID,
        gdrive_file_name=f"{file_name}",
        token_file=OAUTH_TOKEN_FILE,
    )


@dag(
    dag_id="compare_the_market",
    start_date=pendulum.datetime(2022, 11, 1, tz="UTC"),
    # cronjob.batch/ctm-report-daily-upload is currently scheduled at 0 1 * * *
    schedule_interval="0 2 * * *",
    catchup=True,
    default_args={"retries": 0},
    max_active_runs=1,
    max_active_tasks=7,
    tags=["reporting", "daily", "monthly"],
)
def compare_the_market():
    placeholder = EmptyOperator(task_id="generate_report")
    daily_table = update_daily_table()
    daily_upload = upload_daily_report()
    branch = is_first_of_month()
    no_op = EmptyOperator(task_id="no_op")
    monthly_view = update_monthly_view()
    monthly_export = export_monthly_view()
    monthly_upload = upload_monthly_report()

    # Graph
    placeholder >> daily_table >> branch
    daily_table >> daily_upload
    branch >> no_op
    branch >> monthly_view >> monthly_export >> monthly_upload


compare_the_market()
