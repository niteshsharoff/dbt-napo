{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time, datetime\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery, storage\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = True\n",
    "if (dev):\n",
    "    key_path = r\"C:\\Users\\nites\\OneDrive\\Documents\\napo-nitesh-local-ae32-vpcservice-datawarehouse-879a160a28ee.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(credentials=credentials, project=credentials.project_id,)\n",
    "    storage_client = storage.Client(credentials=credentials, project=credentials.project_id,) \n",
    "\n",
    "else: \n",
    "    client = bigquery.Client()\n",
    "    storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "    autodetect=True,\n",
    "    max_bad_records=1,\n",
    "    write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name='data-warehouse-harbour'\n",
    "dataset_name='postgres'\n",
    "path_name='policy-service/{}'.format(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mapping = {\n",
    "    \"policy.breed.json\":\"breed\",\n",
    "   # \"policy.customer.json\":\"customer\",\n",
    "    \"policy.pet.json\":\"pet\",\n",
    "   # \"policy.policy.json\":\"policy\",\n",
    "    \"policy.product.json\":\"product\",\n",
    "    \"policy.productline.json\":\"productline\",\n",
    "    \"policy.payment.json\":\"policy_payment\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(table_mapping,table_id,uri,table_mapping[\"policy.customer.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy.breed.json  was found in today's bucket with file size 346042\n",
      "policy.breed.json LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=ebe25be3-8fcd-4bea-aa6e-cbc4c4d0f37f>\n",
      "policy.pet.json  was found in today's bucket with file size 30134034\n",
      "policy.pet.json LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=01bfd90c-f487-439d-acd8-d034086d5d64>\n",
      "policy.product.json  was found in today's bucket with file size 7219\n",
      "policy.product.json LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=9a0ef1f6-fd1c-4cd9-bbac-be8d39391321>\n",
      "policy.productline.json  was found in today's bucket with file size 304\n",
      "policy.productline.json LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=fd3d2481-d8a3-47b4-92ef-3f2e8ece12d8>\n",
      "policy.payment.json  was found in today's bucket with file size 83874942\n",
      "policy.payment.json LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=43996dfc-cf17-4e72-ba97-769500690a6e>\n"
     ]
    }
   ],
   "source": [
    "for table in table_mapping:\n",
    "    uri = \"gs://{}/{}/{}\".format(bucket_name,path_name,table)\n",
    "    #size_in_bytes = storage_client.bucket.get_blob(uri).size\n",
    "    #print(table,size_in_bytes)\n",
    "    \n",
    "    if (storage_client.get_bucket(bucket_name).get_blob(path_name+'/'+table).exists()):\n",
    "        size=storage_client.get_bucket(bucket_name).get_blob(path_name+'/'+table).size\n",
    "        print(table,' was found in today\\'s bucket with file size',size)\n",
    "        if (size==0):\n",
    "            raise NameError(table+' file size is 0')\n",
    "\n",
    "        table_id = \"ae32-vpcservice-datawarehouse.{}.{}\".format(dataset_name,table_mapping[table])\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri,\n",
    "            table_id,\n",
    "            location=\"EU\",  # To match the destination dataset location\n",
    "            job_config=job_config,\n",
    "            \n",
    "        )\n",
    "        print(table,load_job.result())\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(table,' not found in today\\'s bucket')\n",
    "        raise NameError(table+' not found in bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscription table loading\n",
    "blob = bucket.blob(\"{}/{}\".format(path_name,'policy.subscription.json'))\n",
    "df = pd.read_json(str(blob.download_as_bytes(),'utf-8'), lines=True)\n",
    "df1 = pd.json_normalize(df['fields'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'policyid' in df1.columns:\n",
    "    policy_column = 'policyid' \n",
    "if '_policyid' in df1.columns:\n",
    "    policy_column = '_policyid'\n",
    "    df1.rename(columns={'_policyid':'policyid'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "LoadJob<project=ae32-vpcservice-datawarehouse, location=EU, id=89b1be92-97e2-4164-a9a2-3b42b452585b>\n"
     ]
    }
   ],
   "source": [
    "if (df1.policyid.count()==df.model.count()):\n",
    "    print('True')\n",
    "    df_final = pd.merge(df, df1, left_index=True, right_index=True)\n",
    "    df_final.drop(columns=['fields','failed_payment_events'],inplace=True)\n",
    "\n",
    "job_config2 = bigquery.LoadJobConfig(\n",
    "    autodetect=True,\n",
    "    write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    ")\n",
    "\n",
    "load_job = client.load_table_from_dataframe(\n",
    "    df_final\n",
    "    ,\"ae32-vpcservice-datawarehouse.{}.subscription\".format(dataset_name)\n",
    "    ,job_config=job_config2\n",
    "    ,location='EU'\n",
    ") \n",
    "print(load_job.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "56412660b037274d686266c65699192c47fe0cde2b795cf57673260ecc54f434"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
